{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611183db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing RAG Pipeline ---\n",
      "Initializing the RAG Retriever...\n",
      "Retriever initialized successfully.\n",
      "Initializing the RAG Generator (Ollama)...\n",
      "Generator initialized. Make sure the Ollama server is running.\n",
      "\n",
      "--- Pipeline Initialized ---\n",
      "\n",
      "Evaluating Statement:\n",
      "\"In cases of abdominal gunshot wounds, the liver and intraabdominal vasculature are commonly injured, with involvement rates of 40% and 30% respectively.\"\n",
      "\n",
      "Step 1: Retrieving relevant context...\n",
      "Context retrieved.\n",
      "{'content': 'Traumatic injuries to the abdomen can result from a wide range of '\n",
      "            'etiologies and can lead to life-threatening injuries, multi-organ '\n",
      "            'system dysfunction, and death. Gunshot wounds in the abdominal '\n",
      "            'region can range from minor wounds to severe traumatic injuries '\n",
      "            'depending on the anatomical structures the bullet penetrates. '\n",
      "            'While the leading cause of blunt abdominal trauma-related deaths '\n",
      "            'in the United States in adults ages 15 to 24 is due to motor '\n",
      "            'vehicle collisions, abdominal gunshot wounds account for up to 90 '\n",
      "            'percent of the mortality associated with penetrating abdominal '\n",
      "            'injuries.\\n'\n",
      "            '\\n'\n",
      "            'The most common cause of a penetrating abdominal injury is a stab '\n",
      "            'wound or gunshot wound. In gunshot wounds, due to the '\n",
      "            'high-intensity kinetic energy of the bullet, the pathway is often '\n",
      "            'unpredictable in nature as well as the internal organs that may '\n",
      "            'be affected. The most common organs injured are the small and '\n",
      "            'large bowel at 50% and 40%, respectively. Also, the liver and '\n",
      "            'intraabdominal vasculature are oftentimes injured as well at 40% '\n",
      "            'and 30% involvement, respectively. Although direct abdominal '\n",
      "            'trauma may be caused by a penetrating bullet, there may be '\n",
      "            'shrapnel or fragmentation from the bullets that can disperse into '\n",
      "            'the intra-abdominal cavity also causing injury.',\n",
      " 'section_title': 'Introduction',\n",
      " 'topic_id': 0}\n",
      "\n",
      "Step 2: Generating answer with LLM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/torf/anaconda3/envs/nm-ai-rag/lib/python3.11/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Result ---\n",
      "{'statement_is_true': 1, 'statement_topic': 0}\n",
      "\n",
      "Total evaluation time: 1.39 seconds.\n"
     ]
    }
   ],
   "source": [
    "# test_pipeline_ollama.ipynb\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import faiss\n",
    "import json\n",
    "import requests  # <-- New import\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import time\n",
    "import pprint\n",
    "\n",
    "# RAGRetriever class is unchanged\n",
    "class RAGRetriever:\n",
    "    def __init__(self, index_path=\"faiss_index.bin\", chunks_path=\"clean_chunks.pkl\"):\n",
    "        print(\"Initializing the RAG Retriever...\")\n",
    "        self.index = faiss.read_index(index_path)\n",
    "        with open(chunks_path, \"rb\") as f:\n",
    "            self.chunks = pickle.load(f)\n",
    "        self.model = SentenceTransformer('BAAI/bge-small-en-v1.5', device='cpu')\n",
    "        print(\"Retriever initialized successfully.\")\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 1) -> dict:\n",
    "        query_embedding = self.model.encode(query, normalize_embeddings=True)\n",
    "        query_embedding = query_embedding.reshape(1, -1)\n",
    "        _, indices = self.index.search(query_embedding, k)\n",
    "        return self.chunks[indices[0][0]]\n",
    "\n",
    "# --- THIS IS THE NEW OLLAMA-BASED GENERATOR ---\n",
    "class RAGGenerator:\n",
    "    def __init__(self, model_name=\"mistral\", ollama_host=\"http://localhost:11434\"):\n",
    "        print(\"Initializing the RAG Generator (Ollama)...\")\n",
    "        self.model_name = model_name\n",
    "        self.ollama_api_url = f\"{ollama_host}/api/generate\"\n",
    "        print(\"Generator initialized. Make sure the Ollama server is running.\")\n",
    "\n",
    "    def generate(self, statement: str, context_chunk: dict) -> dict:\n",
    "        context_text = f\"Section: {context_chunk['section_title']}\\n\\n{context_chunk['content']}\"\n",
    "        topic_id = context_chunk['topic_id']\n",
    "        \n",
    "        # We construct a single, clean prompt\n",
    "        prompt = f\"\"\"Context:\n",
    "        ---\n",
    "        {context_text}\n",
    "        ---\n",
    "        Statement: \"{statement}\"\n",
    "\n",
    "        Task: Based ONLY on the provided context, respond with a single, raw JSON object with two keys: \"statement_is_true\" (1 for true, 0 for false) and \"statement_topic\" (the integer topic ID, which is {topic_id}). Do not add any explanation or markdown.\n",
    "        \"\"\"\n",
    "\n",
    "        # Data payload for the Ollama API\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"prompt\": prompt,\n",
    "            \"format\": \"json\",  # Ask Ollama to guarantee the output is JSON\n",
    "            \"stream\": False,\n",
    "            \"options\": {\n",
    "                \"temperature\": 0.0,\n",
    "                \"num_predict\": 60 # Max tokens to generate\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Make the request to the local Ollama server\n",
    "            response = requests.post(self.ollama_api_url, json=payload)\n",
    "            response.raise_for_status() # Raise an exception for bad status codes\n",
    "            \n",
    "            # The response from Ollama with format=\"json\" is already a parsed JSON object\n",
    "            response_json = json.loads(response.json()['response'])\n",
    "            return response_json\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error connecting to Ollama server: {e}\")\n",
    "            return {\"statement_is_true\": -1, \"statement_topic\": -1}\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing JSON from Ollama: {e}\")\n",
    "            print(f\"Raw response was: {response.text}\")\n",
    "            return {\"statement_is_true\": -1, \"statement_topic\": -1}\n",
    "\n",
    "\n",
    "# --- Main Test Function (unchanged) ---\n",
    "def run_local_test():\n",
    "    print(\"--- Initializing RAG Pipeline ---\")\n",
    "    retriever = RAGRetriever()\n",
    "    generator = RAGGenerator()\n",
    "    print(\"\\n--- Pipeline Initialized ---\")\n",
    "\n",
    "    statement = (\n",
    "        \"In cases of abdominal gunshot wounds, the liver and intraabdominal \"\n",
    "        \"vasculature are commonly injured, with involvement rates of 40% and \"\n",
    "        \"30% respectively.\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nEvaluating Statement:\\n\\\"{statement}\\\"\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(\"\\nStep 1: Retrieving relevant context...\")\n",
    "    retrieved_chunk = retriever.retrieve(statement)\n",
    "    print(\"Context retrieved.\")\n",
    "    pprint.pprint(retrieved_chunk)\n",
    "\n",
    "    print(\"\\nStep 2: Generating answer with LLM...\")\n",
    "    result = generator.generate(statement, retrieved_chunk)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(\"\\n--- Final Result ---\")\n",
    "    pprint.pprint(result)\n",
    "    print(f\"\\nTotal evaluation time: {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# --- Run the test ---\n",
    "run_local_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2aac5f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a46f83fd7f7f437dbf282b6f33fd63e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1897e684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing RAG Pipeline ---\n",
      "Initializing the RAG Retriever...\n",
      "Retriever initialized successfully.\n",
      "Initializing the RAG Generator (Ollama)...\n",
      "Generator initialized. Make sure the Ollama server is running.\n",
      "\n",
      "--- Pipeline Initialized ---\n",
      "\n",
      "Evaluating Statement:\n",
      "\"In cases of abdominal gunshot wounds, the liver and intraabdominal vasculature are commonly injured, with involvement rates of 40% and 30% respectively.\"\n",
      "\n",
      "Step 1: Retrieving relevant context...\n",
      "Context retrieved.\n",
      "{'content': 'Traumatic injuries to the abdomen can result from a wide range of '\n",
      "            'etiologies and can lead to life-threatening injuries, multi-organ '\n",
      "            'system dysfunction, and death. Gunshot wounds in the abdominal '\n",
      "            'region can range from minor wounds to severe traumatic injuries '\n",
      "            'depending on the anatomical structures the bullet penetrates. '\n",
      "            'While the leading cause of blunt abdominal trauma-related deaths '\n",
      "            'in the United States in adults ages 15 to 24 is due to motor '\n",
      "            'vehicle collisions, abdominal gunshot wounds account for up to 90 '\n",
      "            'percent of the mortality associated with penetrating abdominal '\n",
      "            'injuries.\\n'\n",
      "            '\\n'\n",
      "            'The most common cause of a penetrating abdominal injury is a stab '\n",
      "            'wound or gunshot wound. In gunshot wounds, due to the '\n",
      "            'high-intensity kinetic energy of the bullet, the pathway is often '\n",
      "            'unpredictable in nature as well as the internal organs that may '\n",
      "            'be affected. The most common organs injured are the small and '\n",
      "            'large bowel at 50% and 40%, respectively. Also, the liver and '\n",
      "            'intraabdominal vasculature are oftentimes injured as well at 40% '\n",
      "            'and 30% involvement, respectively. Although direct abdominal '\n",
      "            'trauma may be caused by a penetrating bullet, there may be '\n",
      "            'shrapnel or fragmentation from the bullets that can disperse into '\n",
      "            'the intra-abdominal cavity also causing injury.',\n",
      " 'section_title': 'Introduction',\n",
      " 'topic_id': 0}\n",
      "\n",
      "Step 2: Generating answer with LLM...\n",
      "\n",
      "--- Final Result ---\n",
      "{'statement_is_true': 1, 'statement_topic': 0}\n",
      "\n",
      "Total evaluation time: 0.16 seconds.\n"
     ]
    }
   ],
   "source": [
    "# test_pipeline.py\n",
    "\n",
    "import time\n",
    "import pprint\n",
    "def run_local_test():\n",
    "    \"\"\"\n",
    "    Runs a full, end-to-end test of the RAG pipeline on a sample statement.\n",
    "    \"\"\"\n",
    "    # --- 1. Initialize Components ---\n",
    "    # This will take some time as it loads all the models into memory.\n",
    "    print(\"--- Initializing RAG Pipeline ---\")\n",
    "    retriever = RAGRetriever()\n",
    "    # Make sure to have the model available. The library will download it on first run.\n",
    "    generator = RAGGenerator()\n",
    "    print(\"\\n--- Pipeline Initialized ---\")\n",
    "\n",
    "    # --- 2. Define a Sample Statement ---\n",
    "    # This statement comes from the example in your project description.\n",
    "    # It's a good test case because it's specific and factual.\n",
    "    statement = (\n",
    "        \"In cases of abdominal gunshot wounds, the liver and intraabdominal \"\n",
    "        \"vasculature are commonly injured, with involvement rates of 40% and \"\n",
    "        \"30% respectively.\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nEvaluating Statement:\\n\\\"{statement}\\\"\")\n",
    "\n",
    "    # --- 3. Run the RAG Pipeline ---\n",
    "    start_time = time.time()\n",
    "\n",
    "    # a) Retrieve the most relevant context\n",
    "    print(\"\\nStep 1: Retrieving relevant context...\")\n",
    "    retrieved_chunk = retriever.retrieve(statement)\n",
    "    print(\"Context retrieved.\")\n",
    "    pprint.pprint(retrieved_chunk)\n",
    "\n",
    "    # b) Generate the answer using the context\n",
    "    print(\"\\nStep 2: Generating answer with LLM...\")\n",
    "    result = generator.generate(statement, retrieved_chunk)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # --- 4. Display the Results ---\n",
    "    print(\"\\n--- Final Result ---\")\n",
    "    pprint.pprint(result)\n",
    "    print(f\"\\nTotal evaluation time: {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "run_local_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edc58b8",
   "metadata": {},
   "source": [
    "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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0bdc191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "import pathlib\n",
    "\n",
    "def run_evaluation(statements_dir: str, ground_truth_dir: str):\n",
    "    retriever = RAGRetriever()\n",
    "    generator = RAGGenerator()\n",
    "    statements_path = pathlib.Path(statements_dir)\n",
    "    statement_files = sorted(list(statements_path.glob(\"*.txt\")))\n",
    "    \n",
    "    if not statement_files:\n",
    "        print(f\"Error: No statement .txt files found in '{statements_dir}'\")\n",
    "        return\n",
    "\n",
    "    total_statements = len(statement_files)\n",
    "    correct_truth_predictions = 0\n",
    "    correct_topic_predictions = 0\n",
    "\n",
    "    print(f\"\\nStarting evaluation of {total_statements} statements...\")\n",
    "\n",
    "    for statement_file in tqdm(statement_files, desc=\"Evaluating Statements\"):\n",
    "        with open(statement_file, 'r', encoding='utf-8') as f:\n",
    "            statement_text = f.read().strip()\n",
    "\n",
    "        statement_id = statement_file.stem\n",
    "        ground_truth_file = pathlib.Path(ground_truth_dir) / f\"{statement_id}.json\"\n",
    "        \n",
    "        try:\n",
    "            with open(ground_truth_file, 'r', encoding='utf-8') as f:\n",
    "                ground_truth = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"\\n[Warning] Ground truth file not found for {statement_id}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        retrieved_chunk = retriever.retrieve(statement_text)\n",
    "        prediction = generator.generate(statement_text, retrieved_chunk)\n",
    "\n",
    "\n",
    "        if prediction[\"statement_is_true\"] == ground_truth[\"statement_is_true\"]:\n",
    "            correct_truth_predictions += 1\n",
    "        \n",
    "        if prediction[\"statement_topic\"] == ground_truth[\"statement_topic\"]:\n",
    "            correct_topic_predictions += 1\n",
    "\n",
    "    truth_accuracy = (correct_truth_predictions / total_statements) * 100\n",
    "    topic_accuracy = (correct_topic_predictions / total_statements) * 100\n",
    "\n",
    "    print(\"\\n--- Evaluation Complete ---\")\n",
    "    print(f\"Total Statements Evaluated: {total_statements}\")\n",
    "    print(\"\\n--- Accuracy Scores ---\")\n",
    "    print(f\"Statement Truth (True/False) Accuracy: {truth_accuracy:.2f}%\")\n",
    "    print(f\"Statement Topic Accuracy:               {topic_accuracy:.2f}%\")\n",
    "    print(\"-----------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b6ba0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the RAG Retriever...\n",
      "Retriever initialized successfully.\n",
      "Initializing the RAG Generator (Ollama)...\n",
      "Generator initialized. Make sure the Ollama server is running.\n",
      "\n",
      "Starting evaluation of 200 statements...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Statements: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:44<00:00,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Complete ---\n",
      "Total Statements Evaluated: 200\n",
      "\n",
      "--- Accuracy Scores ---\n",
      "Statement Truth (True/False) Accuracy: 72.00%\n",
      "Statement Topic Accuracy:               70.50%\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_STATEMENTS_DIR = \"/home/torf/NM-i-AI-2025-Neural-Networks-Enjoyers/emergency-healthcare-rag/data/train/statements/\"\n",
    "VALIDATION_TRUTH_DIR = \"/home/torf/NM-i-AI-2025-Neural-Networks-Enjoyers/emergency-healthcare-rag/data/train/answers/\"\n",
    "\n",
    "run_evaluation(\n",
    "    statements_dir=VALIDATION_STATEMENTS_DIR,\n",
    "    ground_truth_dir=VALIDATION_TRUTH_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87006f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully initialized NVML for VRAM monitoring.\n",
      "--- Starting Evaluation for Model: mistral ---\n",
      "Initializing the RAG Retriever...\n",
      "Retriever initialized successfully.\n",
      "Initializing RAG Generator with Ollama model: 'mistral'...\n",
      "Generator initialized successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating mistral:   0%|          | 0/200 [00:00<?, ?it/s]/home/torf/anaconda3/envs/nm-ai-rag/lib/python3.11/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Evaluating mistral:  10%|â–ˆ         | 21/200 [00:05<00:34,  5.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | VRAM after statement 20: 29.12 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating mistral:  20%|â–ˆâ–ˆ        | 41/200 [00:10<00:35,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | VRAM after statement 40: 29.12 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating mistral:  30%|â–ˆâ–ˆâ–ˆ       | 60/200 [00:14<00:28,  4.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | VRAM after statement 60: 29.13 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating mistral:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/200 [00:18<00:22,  5.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | VRAM after statement 80: 29.17 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating mistral:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/200 [00:23<00:22,  4.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | VRAM after statement 100: 29.19 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating mistral:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/200 [00:27<00:14,  5.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | VRAM after statement 120: 29.18 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating mistral:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/200 [00:31<00:11,  5.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | VRAM after statement 140: 29.20 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating mistral:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/200 [00:36<00:08,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | VRAM after statement 160: 29.15 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating mistral:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 180/200 [00:40<00:05,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | VRAM after statement 180: 29.11 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating mistral: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:45<00:00,  4.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | VRAM after statement 200: 29.11 GB\n",
      "\n",
      "--- Evaluation Complete ---\n",
      "Model Tested: mistral\n",
      "Total Statements: 200\n",
      "\n",
      "--- Accuracy Scores ---\n",
      "Statement Truth Accuracy: 72.50%\n",
      "Statement Topic Accuracy: 70.50%\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate.py (Final Version with Model Switching and VRAM Monitoring)\n",
    "\n",
    "import json\n",
    "import time\n",
    "import pathlib\n",
    "import requests\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import pynvml  # <-- New import for VRAM monitoring\n",
    "\n",
    "# --- VRAM Monitoring Setup ---\n",
    "try:\n",
    "    pynvml.nvmlInit()\n",
    "    # Assuming you are using the first GPU (device 0)\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "    print(\"Successfully initialized NVML for VRAM monitoring.\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not initialize NVML for VRAM monitoring. VRAM usage will not be shown. Error: {e}\")\n",
    "    handle = None\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "# Suppress common warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"transformers\")\n",
    "\n",
    "# RAGRetriever remains the same\n",
    "class RAGRetriever:\n",
    "    def __init__(self, index_path=\"faiss_index.bin\", chunks_path=\"clean_chunks.pkl\"):\n",
    "        print(\"Initializing the RAG Retriever...\")\n",
    "        self.index = faiss.read_index(index_path)\n",
    "        with open(chunks_path, \"rb\") as f:\n",
    "            self.chunks = pickle.load(f)\n",
    "        self.model = SentenceTransformer('BAAI/bge-small-en-v1.5', device='cpu')\n",
    "        print(\"Retriever initialized successfully.\")\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 1) -> dict:\n",
    "        \"\"\"\n",
    "        Retrieves the top-k most relevant chunk for a given query.\n",
    "        \"\"\"\n",
    "        query_embedding = self.model.encode(query, normalize_embeddings=True)\n",
    "        query_embedding = query_embedding.reshape(1, -1)\n",
    "        _, indices = self.index.search(query_embedding, k)\n",
    "        # We only need the single best chunk for this pipeline\n",
    "        return self.chunks[indices[0][0]]\n",
    "\n",
    "# RAGGenerator is updated to accept a model name\n",
    "class RAGGenerator:\n",
    "    def __init__(self, model_name: str, ollama_host=\"http://localhost:11434\"):\n",
    "        print(f\"Initializing RAG Generator with Ollama model: '{model_name}'...\")\n",
    "        self.model_name = model_name\n",
    "        self.ollama_api_url = f\"{ollama_host}/api/generate\"\n",
    "        # Check if the model exists in Ollama\n",
    "        try:\n",
    "            requests.post(self.ollama_api_url, json={\"model\": model_name, \"prompt\": \"Hi\", \"stream\": False})\n",
    "        except requests.exceptions.RequestException as e:\n",
    "             print(f\"\\n---FATAL ERROR---\\nCould not connect to Ollama server at {ollama_host}.\\nPlease ensure the Ollama application is running.\")\n",
    "             exit() # Exit the script if Ollama isn't running\n",
    "        print(\"Generator initialized successfully.\")\n",
    "\n",
    "\n",
    "    def generate(self, statement: str, context_chunk: dict) -> dict:\n",
    "        context_text = f\"Section: {context_chunk['section_title']}\\n\\n{context_chunk['content']}\"\n",
    "        topic_id = context_chunk['topic_id']\n",
    "        \n",
    "        prompt = f\"\"\"Context:\n",
    "---\n",
    "{context_text}\n",
    "---\n",
    "Statement: \"{statement}\"\n",
    "\n",
    "Task: Based ONLY on the provided context, respond with a single, raw JSON object with two keys: \"statement_is_true\" (1 for true, 0 for false) and \"statement_topic\" (the integer topic ID, which is {topic_id}). Do not add any explanation or markdown.\"\"\"\n",
    "\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"prompt\": prompt,\n",
    "            \"format\": \"json\",\n",
    "            \"stream\": False,\n",
    "            \"options\": {\"temperature\": 0.0, \"num_predict\": 60}\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(self.ollama_api_url, json=payload)\n",
    "            response.raise_for_status()\n",
    "            response_json = json.loads(response.json()['response'])\n",
    "            return response_json\n",
    "        except (requests.exceptions.RequestException, json.JSONDecodeError) as e:\n",
    "            print(f\"\\nError during generation: {e}\")\n",
    "            return {\"statement_is_true\": -1, \"statement_topic\": -1}\n",
    "\n",
    "def get_vram_usage_gb(device_handle):\n",
    "    \"\"\"Returns the used VRAM in gigabytes.\"\"\"\n",
    "    if not device_handle:\n",
    "        return None\n",
    "    info = pynvml.nvmlDeviceGetMemoryInfo(device_handle)\n",
    "    return info.used / (1024**3)\n",
    "\n",
    "def run_evaluation(model_to_test: str, statements_dir: str, ground_truth_dir: str):\n",
    "    print(f\"--- Starting Evaluation for Model: {model_to_test} ---\")\n",
    "    \n",
    "    retriever = RAGRetriever()\n",
    "    generator = RAGGenerator(model_name=model_to_test)\n",
    "\n",
    "    statements_path = pathlib.Path(statements_dir)\n",
    "    statement_files = sorted(list(statements_path.glob(\"*.txt\")))\n",
    "    \n",
    "    total_statements = len(statement_files)\n",
    "    correct_truth = 0\n",
    "    correct_topic = 0\n",
    "    \n",
    "    for i, statement_file in enumerate(tqdm(statement_files, desc=f\"Evaluating {model_to_test}\")):\n",
    "        with open(statement_file, 'r', encoding='utf-8') as f:\n",
    "            statement_text = f.read().strip()\n",
    "\n",
    "        statement_id = statement_file.stem\n",
    "        ground_truth_file = pathlib.Path(ground_truth_dir) / f\"{statement_id}.json\"\n",
    "        \n",
    "        with open(ground_truth_file, 'r', encoding='utf-8') as f:\n",
    "            ground_truth = json.load(f)\n",
    "\n",
    "        retrieved_chunk = retriever.retrieve(statement_text)\n",
    "        prediction = generator.generate(statement_text, retrieved_chunk)\n",
    "\n",
    "        if prediction[\"statement_is_true\"] == ground_truth[\"statement_is_true\"]:\n",
    "            correct_truth += 1\n",
    "        \n",
    "        if prediction[\"statement_topic\"] == ground_truth[\"statement_topic\"]:\n",
    "            correct_topic += 1\n",
    "            \n",
    "        # Print VRAM usage every 20 statements\n",
    "        if (i + 1) % 20 == 0 and handle:\n",
    "            vram = get_vram_usage_gb(handle)\n",
    "            print(f\" | VRAM after statement {i+1}: {vram:.2f} GB\")\n",
    "\n",
    "\n",
    "    truth_accuracy = (correct_truth / total_statements) * 100\n",
    "    topic_accuracy = (correct_topic / total_statements) * 100\n",
    "\n",
    "    print(\"\\n--- Evaluation Complete ---\")\n",
    "    print(f\"Model Tested: {model_to_test}\")\n",
    "    print(f\"Total Statements: {total_statements}\")\n",
    "    print(\"\\n--- Accuracy Scores ---\")\n",
    "    print(f\"Statement Truth Accuracy: {truth_accuracy:.2f}%\")\n",
    "    print(f\"Statement Topic Accuracy: {topic_accuracy:.2f}%\")\n",
    "    print(\"-----------------------\")\n",
    "\n",
    "\n",
    "\n",
    "# ollama pull mistral\n",
    "# ollama pull llama3\n",
    "# ollama pull gemma:7b\n",
    "MODEL_NAME = \"mistral\" \n",
    "\n",
    "VALIDATION_STATEMENTS_DIR = \"/home/torf/NM-i-AI-2025-Neural-Networks-Enjoyers/emergency-healthcare-rag/data/train/statements/\"\n",
    "VALIDATION_TRUTH_DIR = \"/home/torf/NM-i-AI-2025-Neural-Networks-Enjoyers/emergency-healthcare-rag/data/train/answers/\"\n",
    "\n",
    "run_evaluation(\n",
    "    model_to_test=MODEL_NAME,\n",
    "    statements_dir=VALIDATION_STATEMENTS_DIR,\n",
    "    ground_truth_dir=VALIDATION_TRUTH_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5440d8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import faiss\n",
    "import json\n",
    "import requests  # <-- New import\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import time\n",
    "import pprint\n",
    "\n",
    "# RAGRetriever class is unchanged\n",
    "class RAGRetriever:\n",
    "    def __init__(self, index_path=\"faiss_index.bin\", chunks_path=\"clean_chunks.pkl\"):\n",
    "        print(\"Initializing the RAG Retriever...\")\n",
    "        self.index = faiss.read_index(index_path)\n",
    "        with open(chunks_path, \"rb\") as f:\n",
    "            self.chunks = pickle.load(f)\n",
    "        self.model = SentenceTransformer('BAAI/bge-small-en-v1.5', device='cpu')\n",
    "        print(\"Retriever initialized successfully.\")\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 1) -> dict:\n",
    "        query_embedding = self.model.encode(query, normalize_embeddings=True)\n",
    "        query_embedding = query_embedding.reshape(1, -1)\n",
    "        _, indices = self.index.search(query_embedding, k)\n",
    "        return self.chunks[indices[0][0]]\n",
    "\n",
    "# --- THIS IS THE NEW OLLAMA-BASED GENERATOR ---\n",
    "class RAGGenerator:\n",
    "    def __init__(self, model_name=\"mistral\", ollama_host=\"http://localhost:11434\"):\n",
    "        print(\"Initializing the RAG Generator (Ollama)...\")\n",
    "        self.model_name = model_name\n",
    "        self.ollama_api_url = f\"{ollama_host}/api/generate\"\n",
    "        print(\"Generator initialized. Make sure the Ollama server is running.\")\n",
    "\n",
    "    def generate(self, statement: str, context_chunk: dict) -> dict:\n",
    "        context_text = f\"Section: {context_chunk['section_title']}\\n\\n{context_chunk['content']}\"\n",
    "        topic_id = context_chunk['topic_id']\n",
    "        \n",
    "        # We construct a single, clean prompt\n",
    "        prompt = f\"\"\"Context:\n",
    "        ---\n",
    "        {context_text}\n",
    "        ---\n",
    "        Statement: \"{statement}\"\n",
    "\n",
    "        Task: Based ONLY on the provided context, respond with a single, raw JSON object with two keys: \"statement_is_true\" (1 for true, 0 for false) and \"statement_topic\" (the integer topic ID, which is {topic_id}). Do not add any explanation or markdown.\n",
    "        \"\"\"\n",
    "\n",
    "        # Data payload for the Ollama API\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"prompt\": prompt,\n",
    "            \"format\": \"json\",  # Ask Ollama to guarantee the output is JSON\n",
    "            \"stream\": False,\n",
    "            \"options\": {\n",
    "                \"temperature\": 0.0,\n",
    "                \"num_predict\": 60 # Max tokens to generate\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Make the request to the local Ollama server\n",
    "            response = requests.post(self.ollama_api_url, json=payload)\n",
    "            response.raise_for_status() # Raise an exception for bad status codes\n",
    "            \n",
    "            # The response from Ollama with format=\"json\" is already a parsed JSON object\n",
    "            response_json = json.loads(response.json()['response'])\n",
    "            return response_json\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error connecting to Ollama server: {e}\")\n",
    "            return {\"statement_is_true\": -1, \"statement_topic\": -1}\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing JSON from Ollama: {e}\")\n",
    "            print(f\"Raw response was: {response.text}\")\n",
    "            return {\"statement_is_true\": -1, \"statement_topic\": -1}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nm-ai-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
